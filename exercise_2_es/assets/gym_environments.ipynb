{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7a4f2d-e767-4ac0-beb5-57bd12ffd24a",
   "metadata": {},
   "source": [
    "# Introduction Gym Environments\n",
    "\n",
    "In this exercise, we will evolve a controller for the widely used benchmark problem [Bipedal Walker](https://gymnasium.farama.org/environments/box2d/bipedal_walker/) (Brockman et al. 2016). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a122a-5a73-4bfd-9fb4-0eda2f3221a6",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89893f0-055e-46c4-902e-a3b3383901af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T12:43:00.864160Z",
     "iopub.status.busy": "2024-02-15T12:43:00.863367Z",
     "iopub.status.idle": "2024-02-15T12:43:07.089030Z",
     "shell.execute_reply": "2024-02-15T12:43:07.086444Z",
     "shell.execute_reply.started": "2024-02-15T12:43:00.864087Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: swig in /home/.local/lib/python3.11/site-packages (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131c4be4-db0f-41f7-9e78-422d648717b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T12:43:07.095329Z",
     "iopub.status.busy": "2024-02-15T12:43:07.094526Z",
     "iopub.status.idle": "2024-02-15T12:43:13.472826Z",
     "shell.execute_reply": "2024-02-15T12:43:13.469846Z",
     "shell.execute_reply.started": "2024-02-15T12:43:07.095234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/.local/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/jlab-env/lib/python3.11/site-packages (from gymnasium[box2d]) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/jlab-env/lib/python3.11/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/jlab-env/lib/python3.11/site-packages (from gymnasium[box2d]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/.local/lib/python3.11/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/.local/lib/python3.11/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/.local/lib/python3.11/site-packages (from gymnasium[box2d]) (2.5.2)\n",
      "Requirement already satisfied: swig==4.* in /home/.local/lib/python3.11/site-packages (from gymnasium[box2d]) (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e897ec-c95a-4c4c-8779-5b6dda1a4320",
   "metadata": {},
   "source": [
    "## Gym API\n",
    "\n",
    "The Gym API is the standard notation for defining learning environments throught research and industry. It follows the description of a Markov Decision Process (MDP). At each time step, the environment is in some state $s$ and the decision maker (our agent or also called policy) may choose a action $a$. Conditioned on the previous state and the action, the environment is moving into the new state $s'$, giving the agent a corresponding reward $r_t$. This process is illustrated in the following figure:\n",
    "\n",
    "<img src=\"learning_loop.svg\" width=\"500\">\n",
    "\n",
    "Each environment of Gym contains two functions: `step` and `reset`.\n",
    "1. `step` takes an `action` and returns an environment state `observation`. Action corresponds e.g. to motor signals of our robots. Observations describe sensor signals.\n",
    "2. `reset` sets our environment back to an initial state, e.g. when the environment terminates after a maximum number of time steps or truncates when our environment is in a critical state (e.g. the robot falls down and does not know how to get back in a save state).\n",
    "\n",
    "Gym (since 2021 Gymnasium) is frequently used to benchmark novel learning algorithms against each other and provides a large number of different environments e.g. video games with image observation spaces or simulated robots with continuous action spaces.\n",
    "\n",
    "### Examples of environments with discrete action spaces\n",
    "\n",
    "<img src=\"assault.gif\" height=\"200\"><img src=\"breakout.gif\" height=\"200\"><img src=\"montezuma_revenge.gif\" height=\"200\"><img src=\"pong.gif\" height=\"200\"><img src=\"space_invaders.gif\" height=\"200\">\n",
    "\n",
    "### Examples of environments with continuous action spaces\n",
    "<img src=\"reacher.gif\" height=\"100\"/><img src=\"pusher.gif\" height=\"100\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34121502-32fe-4aa9-831c-947940cad3b1",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here is an example for the usage of gym with Python. Note that currently we are randomly selecting actions with `env.action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ba7755-d6c2-46bd-9fbe-9b2e575fee8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T12:43:13.477352Z",
     "iopub.status.busy": "2024-02-15T12:43:13.476551Z",
     "iopub.status.idle": "2024-02-15T12:43:15.006026Z",
     "shell.execute_reply": "2024-02-15T12:43:15.005334Z",
     "shell.execute_reply.started": "2024-02-15T12:43:13.477271Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward at step 0: 2.567178847980938\n",
      "Reward at step 100: 0.6089772605790926\n",
      "Reward at step 200: -2.3555817370705356\n",
      "Reward at step 300: -1.4643772833698165\n",
      "Reward at step 400: 3.5573684844788263\n",
      "Reward at step 500: -1.4525940823439782\n",
      "Reward at step 600: 2.16767366648956\n",
      "Reward at step 700: -0.3106328884129425\n",
      "Reward at step 800: -1.3630319897515324\n",
      "Reward at step 900: -1.8764936616943533\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "environment = gym.make(\"LunarLander-v2\")\n",
    "observation, info = environment.reset(seed=0)\n",
    "\n",
    "for step in range(1000):\n",
    "    # we select randomly an action\n",
    "    action = environment.action_space.sample()  \n",
    "    \n",
    "    # execute the action in environment\n",
    "    observation, reward, terminated, truncated, info = environment.step(action)\n",
    "\n",
    "    # if lander is colliding with moon or flies out of the picture\n",
    "    if terminated or truncated:\n",
    "        observation, info = environment.reset(seed=0)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f\"Reward at step {step}: {reward}\")\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c92544-9add-4b70-ae99-6f0236e6bd1a",
   "metadata": {},
   "source": [
    "Since we select our actions randomly, we do not observe an increase of performance. The goal of our exercise is to use evolution to generate a policy able to solve the environment (by maximising the reward). The following example give us adds a graphical render of the environment simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fb64b5-b45c-4f70-9067-792117ffdd3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T12:43:15.007748Z",
     "iopub.status.busy": "2024-02-15T12:43:15.006942Z",
     "iopub.status.idle": "2024-02-15T12:43:22.564145Z",
     "shell.execute_reply": "2024-02-15T12:43:22.563031Z",
     "shell.execute_reply.started": "2024-02-15T12:43:15.007718Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7afdb59f28494caa8877bb902f2554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from io import BytesIO\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "\n",
    "# Create a widget for displaying the environment\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "\n",
    "# Display the widget\n",
    "display(image_widget)\n",
    "\n",
    "def update_image(env):\n",
    "    img = env.render()\n",
    "    img = Image.fromarray(img)\n",
    "    with BytesIO() as output:\n",
    "        img.save(output, format=\"JPEG\")\n",
    "        image_data = output.getvalue()\n",
    "        image_widget.value = image_data\n",
    "\n",
    "# Run and update environment\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Replace with your action policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    \n",
    "    update_image(env)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42186f-defd-444b-b00c-039efa9d861c",
   "metadata": {},
   "source": [
    "## Vectorized Environments\n",
    "This environments are running one agent in one environment. However, with evolutionary algorithms we will generate populations of different solutions, which will be evaluated in our simulator. Here vectorized environments provided by Gym come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b256df27-a1ca-4965-9769-453beb0c6642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T14:26:23.841121Z",
     "iopub.status.busy": "2024-02-15T14:26:23.836996Z",
     "iopub.status.idle": "2024-02-15T14:26:25.825503Z",
     "shell.execute_reply": "2024-02-15T14:26:25.824341Z",
     "shell.execute_reply.started": "2024-02-15T14:26:23.840929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards at step 0: [-0.22549972 -0.119934  ]\n",
      "Rewards at step 100: [-0.17139345 -0.11991324]\n",
      "Rewards at step 200: [-0.42181716 -0.0340379 ]\n",
      "Rewards at step 300: [ 0.00606177 -0.01138707]\n",
      "Rewards at step 400: [ 0.04695277 -0.05570645]\n",
      "Rewards at step 500: [-0.03434103 -0.15757243]\n",
      "Rewards at step 600: [-0.00609256  0.01684054]\n",
      "Rewards at step 700: [-0.03342197 -0.19368984]\n",
      "Rewards at step 800: [-0.25804086 -0.03672408]\n",
      "Rewards at step 900: [-0.23872143  0.00505408]\n",
      "Rewards at step 1000: [-0.15324709  0.03204482]\n",
      "Rewards at step 1100: [-0.03446048 -0.07784076]\n",
      "Rewards at step 1200: [-0.03603886 -0.07487069]\n",
      "Rewards at step 1300: [-0.00501487 -0.10207238]\n",
      "Rewards at step 1400: [-0.04403143 -0.13639763]\n",
      "Rewards at step 1500: [-0.14361706 -0.06501747]\n",
      "Rewards at step 1600: [-0.0947503  -0.13591045]\n",
      "Rewards at step 1700: [ 0.18460886 -0.20983365]\n",
      "Rewards at step 1800: [-0.06944387 -0.10652284]\n",
      "Rewards at step 1900: [-0.20537487  0.07470934]\n",
      "Rewards at step 2000: [-0.09714866 -0.14178289]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "envs = gym.vector.make(\"BipedalWalker-v3\", num_envs=2, asynchronous=False)\n",
    "observations, infos = envs.reset(seed=42)\n",
    "\n",
    "total_steps = 2000\n",
    "\n",
    "for step in range(total_steps+1):\n",
    "    actions = envs.action_space.sample()\n",
    "    observations, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    if step % 100 == 0 or step == total_steps:\n",
    "        # print(f\"Observations at step {step}: {observations}\")\n",
    "        print(f\"Rewards at step {step}: {rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac0b54-59c8-4ff1-9475-8f4392d48dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
